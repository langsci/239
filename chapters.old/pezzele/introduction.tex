\section{Introduction}
\label{sec:introduction}

\subsection{Classifying compounds}\label{sec:class}

%\todo{We thank the reviewer for all the insightful comments. Note: We highlight the new text in red, and the changes made to account for reviewer's comments are signalled with a margin note.} 

Compounding, namely the mechanism by which two independent words (e.g. \emph{pet}, \emph{food}) combine together to form a novel morphologically-complex word (e.g. \emph{petfood}), is one of the most extensively covered topics in the literature of word formation.\footnote{For a complete and exhaustive overview of compounding, see \cite{OHC}.} On the theoretical level, many linguists have been particularly interested in classifying \emph{compounds} according to various criteria, such as `headedness' (roughly speaking, the position and the characteristics of the compound \emph{head}, the dominant word in the compound, e.g. \emph{food} in \emph{petfood}) \citep{Bloomfield1933,fabb1998}; the presence of a verb or a deverbal noun \citep{marchand1969}; the kind of underlying relation between the \emph{constituent} words, either at a syntactic \citep{Bloomfield1933,bally1950,lees1960,SB2005,baroni2006,dressler2006,sbOHC} or at a semantic level \citep{levi1978,warren1978,fanselow1981}. Though different and pertaining to somehow diverse levels of analysis, these criteria have been traditionally explored and mixed together within the same classification frameworks (see among others \citealt{bauer2001,hasp2002,Booij2005}). As a consequence, many influential proposals distinguish various classes of compounds on the basis of several overlapping properties, that often generate an inconvenient number of subclasses and special cases.

To overcome this issue, \cite{SB2005} proposed a cross-linguistically (and nowadays widely accepted) classification framework based on a single, homogenous criterion, that is, the underlying syntactic relation between the compound constituents. Three main classes of compounds are isolated, namely Subordinate, Attributive, and Coordinate. To illustrate, the compound \emph{doghouse} belongs to the Subordinate class, since the syntactic relation subtending \emph{dog} and \emph{house} is that of subordination. Indeed, the compound can be paraphrased as `the house of the dog'. In contrast, \emph{swordfish} is labeled as Attributive, given that the first constituent, \emph{sword}, acts as an attribute of \emph{fish} (a \emph{swordfish} is `a fish whose nose is shaped like a sword'). Finally, Coordinate compounds are formations like \emph{comedy-drama}, where the first and second constituent are linked by the underlying conjunction `and'.

\subsection{From \emph{word} combination to \emph{conceptual} combination}

Interestingly, a similar tripartition has been proposed in the cognitive psychology literature by works on \emph{conceptual combination} \citep{wisniewski1996,costello2000}, where the focus is on the type of interpretations provided by people to novel combinations. By analyzing the circumlocutions produced by speakers to interpret novel compounds like \emph{zebra-horse}, in fact, three main classes have been traditionally isolated, namely Relation-linking, Property-mapping, and Hybrid or Conjunctive. The first class includes interpretations involving a relation between the two concepts, i.e. a \emph{zebra-horse} is `a horse that preys zebras'. In the second, a property of one concept is mapped to the other, i.e. a \emph{zebra-horse} is `a striped horse'. In the third, the novel concept is interpreted as a hybrid or conjunction of the constituent concepts, i.e. a \emph{zebra-horse} is `a creature having many properties of both horses and zebras'. Though the aim of these works is to study the various interpretations to novel conceptual combinations, without any interest in recognizing classes of \emph{lexicalized} compound words, nevertheless the types they identify are reasonably comparable to the linguistic ones proposed by \cite{SB2005}. In particular, Relation-linking interpretations correspond to compounds included in the Subordinate class, Property-mapping to Attribute, and Hybrid/Conjunctive to Coordinate.

One notable difference is that the \emph{linguistic} classification accounts for lexicalized (or familiar) compounds, whereas the \emph{cognitive} one describes novel combinations which still lack a single, well-defined interpretation. However, we can easily assume that lexicalized compounds are the linguistic realization of a conceptual combination process, in a way that all compounds start out as novel formations and become lexicalized with usage in time \citep{gagne2006}. Consistent with this claim is recent evidence showing that, in the processing of both novel and familiar compounds, an active combination of constituent meanings is routinely in place \citep{gagnesp2009,ji2011,marelli2012,marelli2014}. This would suggest that the difference between novel and familiar compounds is merely in their degree of lexicalization. While the former can still be interpreted by speakers in various ways, the latter have only one possible interpretation, that corresponds to a fixed syntactic relation between the constituent words in \cite{SB2005}.

%It should be mentioned at this point that the position about compositionality in compounding is not unanimous among theorists. That is, although linguists generally agree in considering compounds as the result of some kind of composition of two (or more) words, there is no consensus on what compositionality means when referred to compounds. \cite{libben2014}, for example, argues that the meaning of a compound is extremely rarely determined by the meanings of its constituents. Similarly, \cite{dressler2006} assumes that, due to lexicalization, compounds are never fully transparent in the sense of Frege's Principle of compositionality. More generally, it is often claimed that the meaning of a complex word represents something more than the simple addition of the parts. Similar claims can be found in the literature on conceptual combination, with \cite{gagne2006} arguing that understanding a combined concept involves creating a new concept rather than interpreting the whole as a sum or hybrid of the parts. However, a certain degree of compositionality can be found even in fully opaque compounds (e.g., butterfly), whose meaning is the result of a process of lexicalization that darkened their original, fully-compositional meaning (see \cite{marelli2015}).

The second important difference is that interpretations of novel combinations pertain to the conceptual level, namely they describe relations between the concepts being combined together. As such, the tripartition described above is essentially \emph{semantic}. In contrast, the linguistic classification considered here is based on a purely \emph{syntactic} criterion. Based on the commonalities highlighted above, however, it might be that the two levels of analysis are not mutually exclusive, but possibly related and somehow overlapping. Lexical semantic approaches corroborate this conjecture. \cite{lieber5OHC}, for example, proposed that the different compound types identified by \cite{SB2005} would depend, at least in part, on the intrinsic semantic features of the compound constituents. Moreover, classifications of compounds based on taxonomies of semantic relations reveal a certain degree of overlap between the syntactic and semantic analysis \citep{levi1978}. For example, the semantic relation \texttt{AND} seems hardly distinguishable from the purely syntactic relation of coordination, which is subtended by the underlying conjunction `and'.

\subsection{Aim of the work}

Based on this concurring evidence, we conjecture that various classes of compounds defined at the syntactic level may be also explained in terms of the semantic properties of the compounds and their constituents. In particular, our hypothesis is that measures quantifying the semantic role played by each constituent in contributing to the overall compound meaning, as well as the degree of semantic similarity between the constituents should be effective in predicting different classes. Moreover, we expect these semantic measures to be able to capture different, syntax-based classes without relying on other non-semantic compound properties. Crucially, we do not claim that the distinction is thus purely semantic, making superfluous any categorization focusing on the syntactic relation between the compound constituents. Rather, we believe that the theoretically motivated and widely accepted \emph{discrete} classifications proposed by linguists can be also described in terms of the \emph{continuous}, quantitative aspects of the meaning of compounds and their constituents. In other words, we expect the \emph{quantitative} semantic properties to parallel the \emph{qualitative} grammatical distinctions, thus demonstrating, at the same time, the effectiveness of our proposal and the validity of the linguistic theory.

We experiment with a dataset of English compounds for which annotation based on the classification by \cite{SB2005} (Subordinate, Attributive, Coordinate) is available. To predict each class, we use several semantic variables such as the degree of similarity between the constituents and the individual contribution of each constituent word in determining the meaning of the whole compound. We quantify these measures by using a compositional model of distributional semantics \citep{baroni2010,guevara2010,mitchell2010,zanzotto2010}, following recent evidence proving the effectiveness of this approach in modeling morphological processes such as composition and derivation \citep{marelli2015,gunther2016,marelli2017}.

\subsection{Computational models of meaning}

Based on the core notion that similar words occur in similar contexts \citep{harris1954,firth1957}, distributional semantic models (hence, DSMs) represent lexical meanings by means of vectors encoding the contexts in which words appear in a large corpus. The intuition is that words that occur in similar linguistic contexts (e.g., \emph{cat} and \emph{dog}) should be semantically more similar than words that do not. Typically, this geometric representation is used to quantify the degree of distributional similarity between two words. Given the corresponding vectors, the similarity is computed in terms of their geometric distance, typically the cosine of the angle \citep{turney2010}. In particular, the closer two vectors in the semantic space (i.e., the space populated by all the linguistic vectors), the higher their similarity. Traditional DSMs, such as the pioneering Latent Semantic Analysis (LSA; \citealt{landauer1997}), have been largely used to obtain quantitative estimates of important semantic variables such as the degree of conceptual or topical similarity between two words \citep{pado2007,gagnesp2009,kuperman2009,wang2014}.

%\todo{We added a new subsection where distributional semantic approaches to compound interpretation and compositionality prediction are discussed.}

\subsection{Distributional semantics and compounds}\label{sec:relnew}

In the domain of compounds, distributional semantic approaches have been extensively applied to two main tasks: Noun-noun compound interpretation~\citep{van2013melodi,dima2015automatic,dima2016compositionality,P18-1111,fares2018transfer} and compositionality prediction~\citep{reddy2011empirical,im2013exploring,salehi2014using,salehi2015word,cordeiro2016predicting}. The former task, usually tackled as a classification problem, aims at automatically predicting the \emph{semantic} interpretation of the compound (i.e., the semantic relation tying the constituents). Given the compound \emph{street protest}, for example, a system is trained to predict that the relation holding between the nouns is `locative'. Several datasets of compounds annotated with different numbers of semantic relations have been released for the tasks~\citep{o2007annotating,tratz2010taxonomy}, and various systems capitalizing on distributional representations (usually obtained with neural network architectures; see section~\ref{sec:vectors}) have been recently proposed. Overall, this approach has been proved to be successful in the task, though the performance is shown to be dependent on the number and granularity of semantic relations. As for the latter task, it is focused on predicting the degree of compositionality of a noun-noun compound, namely the extent to which the meaning of the whole depends on the meaning of the constituent words. Various datasets annotated with human judgments have been proposed through time~\citep{reddy2011empirical,roller2013expected,farahmand2015multiword}, and extensive explorations of DSMs in the task have been carried out. Crucially for the purpose of this study, distributional measures of similarity obtained with compositional approaches were found to be highly predictive of human judgments in this task~\citep{reddy2011empirical,im2013exploring,salehi2015word,cordeiro2016predicting}.

\subsection{A compositional approach to compounds}\label{sec:compapp}

Of great interest for the present work, \cite{lynott2001} were the first to employ distributional semantic models to study novel compounds (e.g. \emph{zebra-horse}). In particular, the aim of that work was to test whether a measure of semantic similarity between compound constituents (quantified with LSA) was predictive of both (a) the ease of novel compound comprehension and (b) the distinction between Relation-linking and Property-mapping combinations. To do so, they experimented with novel compounds and their corresponding interpretations as provided by previous works on conceptual combination \citep{wisniewskilove,gagne2000}. Overall, the model was shown to perform remarkably well in all the tasks. \cite{lynott2001}, however, claimed that current distributional models like LSA were not capable of modeling the whole process of conceptual combination. Since they can only quantify the similarity between independent, free-standing words (e.g. \emph{zebra} and \emph{horse}), they are not informative at all about the relation between these words and the resulting compound. As such, they represent static, word-based models of lexical semantics which do not account for the potentially infinite linguistic productivity.


Compositional DSMs (hence, cDSMs) precisely tackle these issues. Aimed at accounting for the compositional nature of language \citep{baronifrege}, these models capitalize on DSM vectors and perform either simple \citep{mitchell2010} or more complex, theoretically inspired operations \citep{baroni2010,guevara2010,zanzotto2010} to \emph{compose} existing lexical entries. By exploiting simple operations (sum, multiplication) or being trained with distributional information about combinations that are already observed in the source corpus, these models can indeed be used to generate meaning representations for both novel and lexicalized formations. Recently, this approach was shown to be effective in modeling morphological processes such as derivation and compounding \citep{marelli2015,gunther2016,marelli2017}. Closely related to the present study, recent work \citep{gunther2016,marelli2017} exploited cDSMs to generate compositional representations of compounds. \cite{marelli2017}, in particular, explored whether a simple but effective regression-based compositional method \citep{guevara2010} can capture the variability in semantic relations between the constituents of novel compounds. This system was shown to be remarkably effective and flexible in capturing relational information. Based on this evidence, in the present work we employ the same model and test it in the task of predicting theoretically motivated, syntax-based classes of compounds.

%The paper is structured as follows: First, we introduce the method, the materials, and the variables used for testing our hypothesis. Then we present the results by means of in-depth analyses. Finally, we discuss the results and discuss their implications. 
